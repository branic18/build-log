## B

#### BERT (Bidirectional Encoder Representations from Transformers)

**BERT**Â is aÂ **pretrained language model**Â developed by Google that helps machines understand the meaning of language by looking atÂ **both the left and right context**Â of every word in a sentence (bidirectional).

It is trained on large amounts of text (like Wikipedia) and can beÂ **fine-tuned**Â for many tasks such as:
- Sentiment analysis
- Question answering
- Text classification
- Named entity recognition

ğŸ” Why it matters: BERT revolutionized natural language processing (NLP) by significantly improving performance on many language understanding tasks â€” without needing to train models from scratch.

#### Bias: 
Learnerâ€™s built-in limits â†’ consistent mistakes. 

## E
#### Embeddings:
A way to represent complex inputs (like words, sentences, or images) asÂ vectorsÂ (lists of numbers).  

ğŸ” Why do we use them?  
- Machine learning modelsÂ canâ€™t directly work with raw text or images.
- EmbeddingsÂ translateÂ those raw inputs intoÂ numerical formatsÂ that capture their meaning, structure, or relationships.

âœ¨ Example:  You can measureÂ similarityÂ between embeddings. This lets modelsÂ understand relationshipsÂ (e.g., king â‰ˆ queen but â‰  apple).

| Word    | Embedding (simplified) |
| ------- | ---------------------- |
| "king"  | [0.8, 0.2, 0.6, ...]   |
| "queen" | [0.8, 0.3, 0.7, ...]   |
| "apple" | [0.1, 0.9, 0.2, ...]   |

ğŸ“¦ In Overton:  
- Pretrained embeddings (like from HuggingFace) can beÂ plugged in as data payloads.
- These embeddings are often trained onÂ huge corpora, which boosts model performance, especially when you donâ€™t have much data.

## F
#### Fit model:
memorized model  
#### Function:
A learned classifier  

## H
#### Hyperparameter
A hyperparameterÂ is a configuration variable that is set manually by a data scientist before the model training begins. Unlike model parameters, such as weights and biases, which are learned automatically from the data during training, hyperparameters are external to the model and control its learning process.Â  

## M
#### ModelÂ generalizations
Whether it learnedÂ real patternsÂ or justÂ memorized.

## O
#### Overfitting
When a model learns theÂ **training data too well**, including its noise or quirks, causing it to performÂ **poorly on new, unseen data**. 

## P
#### Parameter
TheÂ **tunable parts**Â of a model (like weights and biases) that the learning algorithm adjusts to fit the training data.

> ğŸ“Â _All weights are parameters, but not all parameters are weights._

#### Pandas  
Dev tools forÂ AI builders. The fastest way to build and shipÂ general AI agents. Simple APIs, zero DevOps, infinite scale.

## R
#### Runtime performance  
Runtime performanceÂ refers toÂ how efficiently the model runs in production, including:  
- SpeedÂ (how fast it returns predictions)
- LatencyÂ (delay between input and output)
- Resource usageÂ (CPU, memory, power)
Basically, it answers: â€œCan this model respond quickly and efficiently when real users are using it?â€  

## S
#### (Production) SLAs- SLAÂ =Â Service-Level Agreement
This is a contract or standard that defines how well a system must perform in production.  

#### Snorkel
Snorkel AI is a technology startup that empowers data scientists and developers to quickly turn data into accurate and adaptable AI applications with Snorkel Flow, a first-of-its-kind data-centric development platform powered by programmatic labeling. Snorkel Flow reduces the time, cost, and friction of labeling training data so data science and development teams can more easily build and scale AI models to deploy more meaningful applications. Incorporating human judgment into the AI process through subject-matter experts is made more efficient and scalable, leading to more ethical, responsible outcomes. [From Snorkel AI]

#### Supervision
Refers toÂ a machine learning approach where a model is trained on labeled data, with the labels acting as "correct answers" that guide the model to learn the relationship between inputs and outputs.Â This learning process allows the model to make accurate predictions on new, unseen data by comparing its own predictions against these known correct outputs and adjusting itself to minimize errors  

## T
#### Tensor
AÂ **multi-dimensional array**Â of numbersâ€”  that stores data (payloads) and is passed around inside the model

## U

#### Underfitting
When a model isÂ **too simple**Â to capture the patterns in the training data, leading toÂ **poor performance on both training and test data**.

## V
#### Variance
Learnerâ€™s sensitivity to training data â†’ instability.  
How much a modelâ€™s predictionsÂ **change when trained on different datasets**.  
High variance means the model isÂ **too sensitive**Â to the training data and may not generalize well.

## W
#### Weak supervision:  
Instead of using onlyÂ human-labeled data, weak supervision uses:  
- RulesÂ (heuristics)
- Distant supervisionÂ (e.g., using databases or logs)
- Noisy sourcesÂ (user clicks, partial matches, etc.)
These areÂ cheaper, faster, and oftenÂ automatedÂ â€” butÂ less accurate.  

#### Weights
Numbers inside a machine learning model that determine how much influence each input feature has on the model's predictions. They are learned during training.
Weights are also called parameters
Weights / ParametersÂ are the numeric values inside the model that determine how inputs are transformed to outputs. For example, in a neural network, each layer has weights (and biases) which get updated during training.

 








  
